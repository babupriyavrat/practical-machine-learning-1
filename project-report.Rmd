---
title: "Practical Machine Learning Project"
author: "soesilo wijono"
date: "May 19, 2015"
output: html_document
---
## Prediction Assignment

### Background
Using devices such as JawboneUp, NikeFuelBand, and Fitbitit is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in
their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  
   
In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).   

### Loading Data

all blanks, NA, #DIV/0! will be considered  as NA. Load the files into input and validation object. 

```{r}
	input<- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
	validation  <- read.csv("pml-testing.csv",  na.strings = c("NA", "#DIV/0!", ""))
	names(input)

```

 [1] "X"                    "user_name"            "raw_timestamp_part_1" "raw_timestamp_part_2" "cvtd_timestamp"      
 [6] "new_window"           "num_window"           "roll_belt"            "pitch_belt"           "yaw_belt"            
[11] "total_accel_belt"     "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"         "accel_belt_x"        
[16] "accel_belt_y"         "accel_belt_z"         "magnet_belt_x"        "magnet_belt_y"        "magnet_belt_z"       
[21] "roll_arm"             "pitch_arm"            "yaw_arm"              "total_accel_arm"      "gyros_arm_x"         
[26] "gyros_arm_y"          "gyros_arm_z"          "accel_arm_x"          "accel_arm_y"          "accel_arm_z"         
[31] "magnet_arm_x"         "magnet_arm_y"         "magnet_arm_z"         "roll_dumbbell"        "pitch_dumbbell"      
[36] "yaw_dumbbell"         "total_accel_dumbbell" "gyros_dumbbell_x"     "gyros_dumbbell_y"     "gyros_dumbbell_z"    
[41] "accel_dumbbell_x"     "accel_dumbbell_y"     "accel_dumbbell_z"     "magnet_dumbbell_x"    "magnet_dumbbell_y"   
[46] "magnet_dumbbell_z"    "roll_forearm"         "pitch_forearm"        "yaw_forearm"          "total_accel_forearm" 
[51] "gyros_forearm_x"      "gyros_forearm_y"      "gyros_forearm_z"      "accel_forearm_x"      "accel_forearm_y"     
[56] "accel_forearm_z"      "magnet_forearm_x"     "magnet_forearm_y"     "magnet_forearm_z"     "classe"

###cleaning Data
Now, it is imperative to remove NA values. So, we shall use columns only which have atleast one non-NA values

```{r}
input =input[,colSums(is.na(validation)) == 0]
validation=validation[,colSums(is.na(validation)) == 0]
names(input)
```

Now, the columns 1 to 6 is information for reference purpose. 7 doesn't contain the values of features mentioned in the problem, i.e., belt, forearm, arm, and dumbell.
 so, let's remove it.

```{r}
input = input[,8:60]
validation= validation[,8:60]
```

### Preprocessing

Before preprocessing, let's partition the data into training and testing set.

```{r}
library(caret)
set.seed(11051985)
inTrain <- createDataPartition(y=input$classe, p=0.70, list=FALSE)
training  <- input[inTrain,]
testing  <- input[-inTrain,]
```

check if there is any nearzerovalue covariates on training.

```{r}
nsvCol =nearZeroVar(training)
 nsvCol
integer(0)
```


##Plotting Predictors
Let's  create a decision tree model to see which features play a major role in classification.

```{r}
 library(rpart.plot)
 fitModel <- rpart(classe~., data=training, method="class")
 library(rattle)
 fancyRpartPlot(fitModel)
```
 
 we find that the following features play a major role in the classification.
 roll_belt 
 magnet_dumbbell_y
 pitch_forearm
 roll_forearm
 total_accel_dumbbell
 pitch_belt
 magnet_dumbbell_z
 accel_forearm_x
 magnet_arm_y
 yaw_belt
 magnet_belt_z
 magnet_forearm_z

so, let's make a correlation matrix based on these features.

```{r} 
 traincorr= training[,c("roll_belt","magnet_dumbbell_y","pitch_forearm","roll_forearm","total_accel_dumbbell",
 "pitch_belt","magnet_dumbbell_z","accel_forearm_x","magnet_arm_y","yaw_belt","magnet_belt_z","magnet_forearm_z")]
 
library(corrgram) 
 corrgram(traincorr,order=NULL,lower.panel=panel.pie,upper.panel=NULL,text.panel=panel.txt, main="test")
```

 there is only one correlation which is more than 75% i.e. correlation between yaw_belt and roll_belt. However, yaw_belt doesn't seems to be main variable as it appears low in 
 decision tree.

###Creating a Prediction model

Using random forest and taking features only passing correlation matrix, I create the prediction model.

```{r}
 set.seed(11051985)
 Modelfit <- train(classe~roll_belt+magnet_dumbbell_y+pitch_forearm+roll_forearm+total_accel_dumbbell+pitch_belt+magnet_dumbbell_z+accel_forearm_x+magnet_arm_y+magnet_belt_z+magnet_forearm_z,
                  data=training,
                  method="rf",
                  trControl=trainControl(method="cv",number=2),
                  prox=TRUE,
                  verbose=TRUE,
                  allowParallel=TRUE)

predictions <- predict(Modelfit, newdata=testing)
confMatrix<- confusionMatrix(predictions, testing$classe)

confMatrix
```

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1669    6    2    0    0
         B    3 1116    4    3    2
         C    2   15 1017   10    2
         D    0    2    3  950    3
         E    0    0    0    1 1075

Overall Statistics
                                          
               Accuracy : 0.9901          
                 95% CI : (0.9873, 0.9925)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9875          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9970   0.9798   0.9912   0.9855   0.9935
Specificity            0.9981   0.9975   0.9940   0.9984   0.9998
Pos Pred Value         0.9952   0.9894   0.9723   0.9916   0.9991
Neg Pred Value         0.9988   0.9952   0.9981   0.9972   0.9985
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2836   0.1896   0.1728   0.1614   0.1827
Detection Prevalence   0.2850   0.1917   0.1777   0.1628   0.1828
Balanced Accuracy      0.9976   0.9886   0.9926   0.9919   0.9967


###Estimation of out-of-sample error rate


```{r}
errRate = sum(testing$classe!= predictions)/ length(predictions)
errRate 
```
The  out-of-sample error rate is  00.9855565%
 
 
###submission

using validation file to create submission file.


```{r}
> predictions<- predict(Modelfit,newdata=validation)
> validation$classe <- predictions
> submission<-data.frame(problem_id = validation$problem_id,classe= predictions)
> write.csv(submission,file="pml-submission.csv", row.names=FALSE)
> answers=validation$classe
> n=length(answers)
> for(i in 1:n){
+     filename = paste0("problem_",i,".txt")
+     write.table(answers[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
+ }
```

Below is the answer generated.

problem_id	classe
1	B
2	A
3	B
4	D
5	A
6	E
7	D
8	B
9	A
10	A
11	B
12	C
13	B
14	A
15	E
16	E
17	A
18	B
19	B
20	B
	








